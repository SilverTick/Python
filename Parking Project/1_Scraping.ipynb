{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Scraping\n",
    "This is a simple program to scrap carpark information from a website using Pandas and BeautifulSoup and return it in csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up dictionary\n",
    "keys = ['Carpark Name', 'Carpark Address', 'Mon-Fri before 5/6pm', 'Mon-Fri after 5/6pm', 'Sat', 'Sun/Public Holiday']\n",
    "d = collections.OrderedDict((key,[]) for key in keys)\n",
    "\n",
    "# Set up variables\n",
    "full_table = 0\n",
    "location = 'na'\n",
    "address = 'na'\n",
    "url = 'na'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions: web scraping, saving to dataframe, saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes the url and returns the parking fees of the carpark\n",
    "\"\"\"\n",
    "    \n",
    "def readurl(url):\n",
    "    dfs = pd.read_html(url)\n",
    "    data = dfs[6] # dataframe 6 contains the required info\n",
    "    global full_table\n",
    "    full_table = data.iloc[:,0] # required info is in first column\n",
    "    return full_table\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes the url and returns the location and address of the carpark\n",
    "\"\"\"\n",
    "\n",
    "def add_and_loc(url):\n",
    "    resp = requests.get(url)\n",
    "    html_doc = resp.text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    title = soup.find(class_=\"grayboxborder\") # this class contains the table of information\n",
    "    global location\n",
    "    location = title.text.strip()\n",
    "    loc = location.split(\"\\n\")\n",
    "    location = loc[0]\n",
    "    global address\n",
    "    address = loc[1].rstrip()\n",
    "    return location, address\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function appends all the information to the dictionary\n",
    "\"\"\"\n",
    "\n",
    "def append(full_table, location, address):\n",
    "    global d\n",
    "    d['Carpark Name'].append(location)\n",
    "    d['Carpark Address'].append(address)\n",
    "    d['Mon-Fri before 5/6pm'].append(full_table[1])\n",
    "    d['Mon-Fri after 5/6pm'].append(full_table[3])\n",
    "    d['Sat'].append(full_table[5])\n",
    "    d['Sun/Public Holiday'].append(full_table[7])\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "This function passes the dictionary into a dataframe and saves it into a csv file\n",
    "\"\"\"\n",
    "\n",
    "def convert(dictionary, n):\n",
    "    df = pd.DataFrame.from_dict(dictionary, orient='index')\n",
    "    dft = df.transpose()\n",
    "    dft.to_csv('Parking_'+str(n)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0,10):\n",
    "    for i in range( 1+(100*n) , 101+(100*n) ): # there are 926 webpages\n",
    "        try:\n",
    "            front_url = 'http://www.sgcarmart.com/news/carpark_index.php?ID='\n",
    "            back_url = '&LOC=all&TYP=carpark&SRH=#carparkrates'\n",
    "            url = front_url + str(\"%03d\" % i) + back_url\n",
    "            readurl(url)\n",
    "            add_and_loc(url)\n",
    "            append(full_table,location,address)\n",
    "        except AttributeError: # skip blank/error pages and returns url for reference\n",
    "            print(url + ' attr')\n",
    "            i += 1\n",
    "            continue\n",
    "        except ValueError: # skip blank/error pages and returns url for reference\n",
    "            print(url + ' val')\n",
    "            i += 1\n",
    "            continue\n",
    "        except IndexError: # skip blank/error pages and returns url for reference\n",
    "            print(url + ' index')\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        convert(d,n) # creates a csv for every 100 in case of error midway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column (Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load csv\n",
    "full_df = pd.read_csv('Parking_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new column ['Info']\n",
    "full_df['Info'] = '<b>Carpark: </b>' + full_df['Carpark Name']\n",
    "full_df['Info'] += '\\n<b>Address: </b>' + full_df['Carpark Address']\n",
    "full_df['Info'] += '\\n<b>Mon-Fri before 5/6pm: </b>' + full_df['Mon-Fri before 5/6pm']\n",
    "full_df['Info'] += '\\n<b>Mon-Fri after 5/6pm: </b>' + full_df['Mon-Fri after 5/6pm']\n",
    "full_df['Info'] += '\\n<b>Sat: </b>' + full_df['Sat']\n",
    "full_df['Info'] += '\\n<b>Sun/ Public Holiday: </b>' + full_df['Sun/Public Holiday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data: remove carparks not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchfor = ['Carpark closed', 'Private Car Park', 'Carpark not in use', 'Season Parking Only','Building Demolished']\n",
    "clean_df = full_df[~full_df['Mon-Fri before 5/6pm'].str.contains('|'.join(searchfor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "clean_df.to_csv('Parking.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
